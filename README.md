# ğŸ Proyecto Inicial - Middleware ETL ğŸ

En este README encontrarÃ¡s los pasos iniciales para comenzar a trabajar con el proyecto principal y utilizarlo de manera correcta en tu entorno de trabajo; pero como bien dice el nombre, este proyecto es una fase inicial. Esto quiere dar a entender que el proyecto puede cambiar la metodologÃ­a en como se descargarÃ­a o de por sÃ­, un cambio total de la estructura de esta. 

## Conociendo nuestro proyecto ğŸ§

Para comenzar a trabajar con el proyecto necesitamos saber previamente las herramientas que utilizaremos, como tambiÃ©n tener nocion de que versiÃ³n estamos usando. AquÃ­ una muestra de las dependencias mas importantes que usaremos:

```bash
Python: 3.12.4
Apache Airflow: 2.10.0
Great Epectations: 0.18.19
â€‹pandas: 2.1.4
Pyenv: 2.4.10
SqlAlchemy: 1.4.52
```
>[!NOTE]
>Estas dependencias son especificamente las que Airflow recomienda usar con python 3.12 y Apache-Airflow 2.10.0

Ahora se mostrarÃ¡ la estructura de nuestro proyecto para un mejor entendimiento de la guia de instalaciÃ³n.
```bash
.
â””â”€â”€ middleware_capstone/
    â”œâ”€â”€ workflow_management/
    â”‚   â”œâ”€â”€ gx/
    â”‚   â”‚   â”œâ”€â”€ checkpoints/
    â”‚   â”‚   â”œâ”€â”€ expectations/
    â”‚   â”‚   â”‚   â””â”€â”€ my_expectation.json
    â”‚   â”‚   â”œâ”€â”€ plugins/
    â”‚   â”‚   â”œâ”€â”€ profilers/
    â”‚   â”‚   â”œâ”€â”€ uncommitted/
    â”‚   â”‚   â””â”€â”€ great_expectations.yml
    â”‚   â””â”€â”€ dags/
    â”‚       â””â”€â”€ dag_cosume_validate.py
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ modules/
    â”‚   â”‚   â”œâ”€â”€ extract/
    â”‚   â”‚   â”‚   â”œâ”€â”€ data_extractor.py
    â”‚   â”‚   â”œâ”€â”€ transform/
    â”‚   â”‚   â”‚   â””â”€â”€ data_transformation.py
    â”‚   â”‚   â””â”€â”€ load/
    â”‚   â”‚       â””â”€â”€ data_loader.py
    â”‚   â””â”€â”€ common/
    â”‚       â”œâ”€â”€ models/
    â”‚       â”‚   â””â”€â”€ model.py
    â”‚       â”œâ”€â”€ script_sql/
    â”‚       â””â”€â”€ utilities/
    â”‚             â””â”€â”€ data_cleaner.py
    â”œâ”€â”€ logs/
    â”‚   â”œâ”€â”€ dag_id=*
    â”‚   â”œâ”€â”€ dag_procesor_manager
    â”‚   â””â”€â”€ scheduler
    â”œâ”€â”€ .env
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ airflow.cfg
    â”œâ”€â”€ config.sh
    â””â”€â”€ requirements.txt
```
## Paso 1ï¸âƒ£ - InstalaciÃ³n â˜„ï¸
Comenzaremos descargando el proyecto directamente desde este repositorio, para eso, ejecutaremos el siguiente comando:

```bash
git clone https://github.com/Jhopcel/Middleware-ETL-APT.git
```

Una vez clonado el proyecto desde nuestro repositorio, ingresamos a la carpeta correspondiente 
del Proyecto, para esto navegaremos hacia la carpeta â€œmiddleware_capstoneâ€

Ya dentro, crearemos un ambiente virtual para descargar nuestras dependencias y hacer funcionar el proyecto, para esto ejecutamos el siguiente comando:

```bash
pyenv virtualenv 3.12.4 <nombre_ambiente>

#Ejemplo: 

pyenv virtualenv 3.12.4 my_env
```

Ahora comenzaremos con la descarga de las dependencias del proyecto que se encuentra en requirements.txt, para esto ejecutamos el siguiente comando:

```bash
pip install -r requirements.txt --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.12.txt"
```

## Paso 2ï¸âƒ£ - ConfiguraciÃ³n de nuestro entorno ğŸ”¨
Comenzamos configurando nuestras variables de entorno para la correcta ejecuciÃ³n de nuestros DAGs, para esto seguiremos la misma estructura que hay en ".env_example" que se encuentra en la carpeta, aquÃ­ te muestro de igual manera cuÃ¡les son las variables de entorno que utilizaremos:

```markdown
CONNECTION_TO_DB="" #Esta es la conexiÃ³n hacia tu base de datos, la sintaxis es la siguiente: motorDB://userName:password@host/nameDatabase

LOCAL_CONNECTION_TO_METADATA_DB="" #Esta es la conexiÃ³n por defecto y que se aloja en la raiz del proyecto, la sintaxis default es: sqlite:///home/user/my_proyect/airflow.db
```

**Ahora vamos a necesitar indicarle Airflow donde estÃ¡ nuestro proyecto y sus configuraciones. Para esto ejecutaremos un archivo ".sh", que nos permitirÃ¡ hacer esto de manera automÃ¡tica. Al ejecutar el archivo a demÃ¡s de crear nuestras variables de entorno, tambiÃ©n crearÃ¡ una variable con una "Cadena de conexiÃ³n" para la base de datos de Airflow, esta cadena es un paso importante, ya que dentro tendrÃ¡ todos los metadatos que Airflow necesita para funcionar. Esta "cadena de conexiÃ³n" se configura en el archivo ```.env```, especÃ­ficamente la variable "LOCAL_CONNECTION_TO_METADATA_DB". Si no cuentas con una conexiÃ³n a la base de datos, solo deja la variable en blanco, y esto te conectarÃ¡ a una base de datos local, gracias al archivo "config.sh"**

**Para ejecutar el archivo solo ingresamos el siguiente comando:**

```bash
source config.sh
```

>Â¡Importante!, por defecto Airflow asigna la carpeta ```/airflow``` como predeterminado, pero si tu carpeta raÃ­z tiene un nombre diferente y no ejecutaste el archivo ```config.sh```; cuando ejecutes algun comando exclusivo de airflow, este automaticamente crearÃ¡ la carpeta /airflow y se descargarÃ¡ todo en esa direccion, pero no en la tuya.

### ConfiguraciÃ³nes BBDD
Ahora instalaremos las dependencias de nuestra base de datos de Airflow y la conexiÃ³n mediante SqlAchemy. Para esto tendremos dos opciones de motores de base de datos:
* **OpciÃ³n 1**: **MySQL**
  
  ```bash
    sudo apt-get install -y build-essential libmysqlclient-dev
  ```
  ```bash
    sudo apt-get install mysql-server
  ```
  ```bash
    pip install mysql-connector-python==9.0.0
  ```
  ```bash
    pip install mysqlclient==2.2.4


* **OpciÃ³n 2**: **PostgreSQL (Por defecto y con la que esta construida el proyecto")** 
  ```bash
  sudo apt install postgresql postgresql-contrib
  ```
* Ingresamos a la consola con el usuario por defecto, que es "postgres":
  ```bash
  sudo -u postgres psql
  ```
* Cambiamos la contraseÃ±a por defecto e ingresamos una nuestra:
  ```bash
  \password
  ```
* Creamos la base de datos:
  ```bash
  CREATE DATABASE test_middleware;
  ```
 >Recuerda que este es un ejemplo, tÃº puedes cambiar el nombre de la base de datos a tu conveniencia

* Â¡Listo!, ahora salimos de la consola con:
  ```bash
  \q
  ```
## Paso 3ï¸âƒ£ - Iniciando nuestro DAG ğŸŸ¢

Luego de realizar los pasos anteriores, comenzaremos iniciando el servidor de Airflow para poder tener una mejor percepciÃ³n de nuestras tareas que se ejecutarÃ¡n. Para iniciar el servidor de manera correcta se necesitan 2 comandos que se deben ejecutar, para esto nos podemos apoyar de "visual studio code", ya que ofrece varias instancias de consolas. AquÃ­ te muestro los pasos correspondientes para iniciar el servidor dentro de "visual studio code":

En nuestro WSL2 y dentro de la carpeta raiz del proyecto, ejecutaremos un comando que abrirÃ¡ "visual studio code" en nuestro sistema local:

```bash
code .
```

Ya con el "Visual Estudio Code" abierto, nos posicionaremos en la parte superior, especificamente en la opciÃ³n de "terminal" y seleccionamos la opciÃ³n de "Nuevo Terminal".

>[!NOTE]
>El paso anterior nos abrirÃ¡ una terminal con la ruta de la carpeta en nuestro entorno local.

Ahora necesitamos nuestra instancia en WSL dentro de VSC, para esto nos vamos al sÃ­mbolo de + y seleccionamos nuestra instancia actual.

Una vez hecho estos pasos podemos crear tantas instancias queramos. Pero Â¡OJO! que estas instancias son diferentes a la sesiÃ³n actual de nuestro WSL, ya que no mantiene las variables de entorno que creamos en la instancia principal, si queremos mantener la misma variable de entorno que asignamos anteriormente, solo debemos ejecutar el archivo ```config.sh```

Ya que tenemos listo y configurado nuestro entorno, comenzaremos a ejecutar los comandos para iniciar el servidor. Estos comandos son los siguientes:

```bash
airflow webserver --port 8080
```
```bash
airflow scheduler
```
>[!NOTE]
>Recurda que estos comandos se deben ejecutar en diferentes terminales.

Por ultimo, ingresaremos al servidor mediante cualquier navegador de nuestro sistema local, esto escribiendo el siguiente url:

```bash
http://localhost:8080/
```
Â¡Listo!, ya estarias dentro del servidor web y observando los DAGs que se encuentran en nuestro proyecto.
